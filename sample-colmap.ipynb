{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57fb304",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:25:01.949460Z",
     "iopub.status.busy": "2024-05-12T18:25:01.949144Z",
     "iopub.status.idle": "2024-05-12T18:25:03.359366Z",
     "shell.execute_reply": "2024-05-12T18:25:03.358255Z"
    },
    "papermill": {
     "duration": 1.419934,
     "end_time": "2024-05-12T18:25:03.361599",
     "exception": false,
     "start_time": "2024-05-12T18:25:01.941665",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-13T13:29:00.575286623Z",
     "start_time": "2024-05-13T13:29:00.380553922Z"
    }
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/kaggle'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(new_test_fold):\n\u001B[1;32m      9\u001B[0m     shutil\u001B[38;5;241m.\u001B[39mrmtree(new_test_fold)\n\u001B[0;32m---> 10\u001B[0m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmakedirs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_test_fold\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m list_scene_test \u001B[38;5;241m=\u001B[39m [p \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m Path(old_test_fold)\u001B[38;5;241m.\u001B[39miterdir() \u001B[38;5;28;01mif\u001B[39;00m p\u001B[38;5;241m.\u001B[39mis_dir()]\n\u001B[1;32m     13\u001B[0m user_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/kaggle/input/image-matching-challenge-2024/sample_submission.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/imc/lib/python3.10/os.py:215\u001B[0m, in \u001B[0;36mmakedirs\u001B[0;34m(name, mode, exist_ok)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m head \u001B[38;5;129;01mand\u001B[39;00m tail \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m path\u001B[38;5;241m.\u001B[39mexists(head):\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 215\u001B[0m         \u001B[43mmakedirs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhead\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexist_ok\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileExistsError\u001B[39;00m:\n\u001B[1;32m    217\u001B[0m         \u001B[38;5;66;03m# Defeats race condition when another thread created the path\u001B[39;00m\n\u001B[1;32m    218\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/imc/lib/python3.10/os.py:215\u001B[0m, in \u001B[0;36mmakedirs\u001B[0;34m(name, mode, exist_ok)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m head \u001B[38;5;129;01mand\u001B[39;00m tail \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m path\u001B[38;5;241m.\u001B[39mexists(head):\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 215\u001B[0m         \u001B[43mmakedirs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhead\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexist_ok\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileExistsError\u001B[39;00m:\n\u001B[1;32m    217\u001B[0m         \u001B[38;5;66;03m# Defeats race condition when another thread created the path\u001B[39;00m\n\u001B[1;32m    218\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/imc/lib/python3.10/os.py:225\u001B[0m, in \u001B[0;36mmakedirs\u001B[0;34m(name, mode, exist_ok)\u001B[0m\n\u001B[1;32m    223\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 225\u001B[0m     \u001B[43mmkdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001B[39;00m\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m exist_ok \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m path\u001B[38;5;241m.\u001B[39misdir(name):\n",
      "\u001B[0;31mPermissionError\u001B[0m: [Errno 13] Permission denied: '/kaggle'"
     ]
    }
   ],
   "source": [
    "import shutil, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "home_folder = \"/kaggle/working/\"\n",
    "new_test_fold = home_folder + \"test/\"\n",
    "old_test_fold = '/kaggle/input/image-matching-challenge-2024/test/'\n",
    "if os.path.exists(new_test_fold):\n",
    "    shutil.rmtree(new_test_fold)\n",
    "os.makedirs(new_test_fold)\n",
    "list_scene_test = [p for p in Path(old_test_fold).iterdir() if p.is_dir()]\n",
    "\n",
    "user_df = pd.read_csv('/kaggle/input/image-matching-challenge-2024/sample_submission.csv')\n",
    "orin_image_path = user_df['image_path'].to_list()\n",
    "print(\"len sbmission: \", len(user_df))\n",
    "for scene in list_scene_test:\n",
    "    scene = str(scene)\n",
    "    scene_train = os.path.join(\"/kaggle/input/custom-dataset/train/train/\", os.path.basename(scene))\n",
    "    save_new_test = os.path.join(new_test_fold, os.path.basename(scene), \"images\")\n",
    "    if os.path.exists(scene_train):\n",
    "        os.makedirs(save_new_test, exist_ok = True)\n",
    "        images_training = os.listdir(scene_train+\"/images\")\n",
    "        images_test = os.listdir(scene+\"/images\")\n",
    "        num_add = min(50, len(images_test))\n",
    "        add_imgs = np.random.choice(images_training, num_add)\n",
    "    #     add_imgs = np.random.choice(images_training, 2)\n",
    "\n",
    "        for images in add_imgs:\n",
    "            src = os.path.join(scene_train, \"images\", images)\n",
    "            dst = os.path.join(save_new_test, images)\n",
    "            image_path = dst.replace(home_folder, \"\")\n",
    "            if image_path in user_df['image_path'].to_list():\n",
    "                continue\n",
    "            try:\n",
    "                shutil.copy(src, dst)\n",
    "                new_row = pd.Series({'image_path': image_path, 'dataset': os.path.basename(scene), 'scene': os.path.basename(scene), \"rotation_matrix\":\"\", \"translation_vector\":\"\"})\n",
    "                user_df.loc[len(user_df)] = new_row\n",
    "            except:\n",
    "                print(\"cannot create new\")\n",
    "print(\"len after\", len(user_df))\n",
    "for dirpath, dirnames, filenames in os.walk(new_test_fold):\n",
    "    # Skip the root directory\n",
    "    if dirpath == new_test_fold:\n",
    "        continue\n",
    "\n",
    "    num_files = len(filenames)\n",
    "    print(f\"Folder: {os.path.relpath(dirpath, new_test_fold)} - Number of files: {num_files}\")\n",
    "user_df.to_csv(\"/kaggle/working/sample_submission.csv\", index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "268e763c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-12T18:25:03.375948Z",
     "iopub.status.busy": "2024-05-12T18:25:03.375661Z",
     "iopub.status.idle": "2024-05-12T18:25:11.045606Z",
     "shell.execute_reply": "2024-05-12T18:25:11.044486Z"
    },
    "papermill": {
     "duration": 7.67963,
     "end_time": "2024-05-12T18:25:11.047918",
     "exception": false,
     "start_time": "2024-05-12T18:25:03.368288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "kornia is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.3\r\n",
      "    Uninstalling kornia_rs-0.1.3:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.3\r\n",
      "Successfully installed kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/* /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ba106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:25:11.063718Z",
     "iopub.status.busy": "2024-05-12T18:25:11.062859Z",
     "iopub.status.idle": "2024-05-12T18:25:31.849212Z",
     "shell.execute_reply": "2024-05-12T18:25:31.848354Z"
    },
    "papermill": {
     "duration": 20.796421,
     "end_time": "2024-05-12T18:25:31.851338",
     "exception": false,
     "start_time": "2024-05-12T18:25:11.054917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from typing import Any\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# CV/MLe\n",
    "import cv2\n",
    "import torch\n",
    "from torch import Tensor as T\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import LightGlue, ALIKED\n",
    "from lightglue.utils import load_image, rbd\n",
    "\n",
    "# 3D reconstruction\n",
    "import pycolmap\n",
    "\n",
    "# Data importing into colmap\n",
    "import sys\n",
    "\n",
    "# Provided by organizers\n",
    "# import sys\n",
    "# sys.path.append(\"/kaggle/input/custom-dataset\")\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "\n",
    "\n",
    "def arr_to_str(a):\n",
    "    \"\"\"Returns ;-separated string representing the input\"\"\"\n",
    "    return \";\".join([str(x) for x in a.reshape(-1)])\n",
    "\n",
    "def load_torch_image(file_name, device=torch.device(\"cpu\")):\n",
    "    \"\"\"Loads an image and adds batch dimension\"\"\"\n",
    "    img = K.io.load_image(file_name, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(device)\n",
    "list_scene_test = [p for p in Path(\"/kaggle/input/image-matching-challenge-2024/test/\").iterdir() if p.is_dir()]\n",
    "DEBUG = len(list_scene_test) == 1\n",
    "print(\"DEBUG:\", DEBUG)\n",
    "print(list_scene_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8aa2ba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:25:31.866888Z",
     "iopub.status.busy": "2024-05-12T18:25:31.866365Z",
     "iopub.status.idle": "2024-05-12T18:25:31.880533Z",
     "shell.execute_reply": "2024-05-12T18:25:31.879717Z"
    },
    "papermill": {
     "duration": 0.024009,
     "end_time": "2024-05-12T18:25:31.882382",
     "exception": false,
     "start_time": "2024-05-12T18:25:31.858373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed_images(\n",
    "    paths: list[Path],\n",
    "    model_name: str,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> T:\n",
    "    \"\"\"Computes image embeddings.\n",
    "    \n",
    "    Returns a tensor of shape [len(filenames), output_dim]\n",
    "    \"\"\"\n",
    "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).eval().to(device)\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for i, path in tqdm(enumerate(paths), desc=\"Global descriptors\"):\n",
    "        image = load_torch_image(path)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=image, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs) # last_hidden_state and pooled\n",
    "            \n",
    "            # Max pooling over all the hidden states but the first (starting token)\n",
    "            # To obtain a tensor of shape [1, output_dim]\n",
    "            # We normalize so that distances are computed in a better fashion later\n",
    "            embedding = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=-1, p=2)\n",
    "            \n",
    "        embeddings.append(embedding.detach().cpu())\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "def get_pairs_exhaustive(lst: list[Any]) -> list[tuple[int, int]]:\n",
    "    \"\"\"Obtains all possible index pairs of a list\"\"\"\n",
    "    return list(itertools.combinations(range(len(lst)), 2))            \n",
    "    \n",
    "def get_image_pairs(\n",
    "    paths: list[Path],\n",
    "    model_name: str,\n",
    "    similarity_threshold: float = 0.6,\n",
    "    tolerance: int = 1000,\n",
    "    min_matches: int = 20,\n",
    "    exhaustive_if_less: int = 20,\n",
    "    p: float = 2.0,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> list[tuple[int, int]]:\n",
    "    \"\"\"Obtains pairs of similar images\"\"\"\n",
    "    if len(paths) <= exhaustive_if_less:\n",
    "        return get_pairs_exhaustive(paths)\n",
    "    \n",
    "    matches = []\n",
    "    \n",
    "    # Embed images and compute distances for filtering\n",
    "    embeddings = embed_images(paths, model_name, device)\n",
    "    distances = torch.cdist(embeddings, embeddings, p=p)\n",
    "    \n",
    "    # Remove pairs above similarity threshold (if enough)\n",
    "    mask = distances <= similarity_threshold\n",
    "    image_indices = np.arange(len(paths))\n",
    "    \n",
    "    for current_image_index in range(len(paths)):\n",
    "        mask_row = mask[current_image_index]\n",
    "        indices_to_match = image_indices[mask_row]\n",
    "        \n",
    "        # We don't have enough matches below the threshold, we pick most similar ones\n",
    "        if len(indices_to_match) < min_matches:\n",
    "            indices_to_match = np.argsort(distances[current_image_index])[:min_matches]\n",
    "            \n",
    "        for other_image_index in indices_to_match:\n",
    "            # Skip an image matching itself\n",
    "            if other_image_index == current_image_index:\n",
    "                continue\n",
    "            \n",
    "            # We need to check if we are below a certain distance tolerance \n",
    "            # since for images that don't have enough matches, we picked\n",
    "            # the most similar ones (which could all still be very different \n",
    "            # to the image we are analyzing)\n",
    "            if distances[current_image_index, other_image_index] < tolerance:\n",
    "                # Add the pair in a sorted manner to avoid redundancy\n",
    "                matches.append(tuple(sorted((current_image_index, other_image_index.item()))))\n",
    "                \n",
    "    return sorted(list(set(matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a1ce90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:25:31.897397Z",
     "iopub.status.busy": "2024-05-12T18:25:31.896948Z",
     "iopub.status.idle": "2024-05-12T18:25:31.905504Z",
     "shell.execute_reply": "2024-05-12T18:25:31.904727Z"
    },
    "papermill": {
     "duration": 0.017985,
     "end_time": "2024-05-12T18:25:31.907386",
     "exception": false,
     "start_time": "2024-05-12T18:25:31.889401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_keypoints(\n",
    "    paths: list[Path],\n",
    "    feature_dir: Path,\n",
    "    num_features: int = 4096,\n",
    "    resize_to: int = 1024,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> None:\n",
    "    \"\"\"Detects the keypoints in a list of images with ALIKED\n",
    "    \n",
    "    Stores them in feature_dir/keypoints.h5 and feature_dir/descriptors.h5\n",
    "    to be used later with LightGlue\n",
    "    \"\"\"\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    \n",
    "    extractor = ALIKED(\n",
    "        max_num_keypoints=num_features, \n",
    "        detection_threshold=0.01, \n",
    "        resize=resize_to\n",
    "    ).eval().to(device, dtype)\n",
    "    \n",
    "    feature_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"computing detect_keypoints\", len(paths))\n",
    "    with h5py.File(feature_dir / \"keypoints.h5\", mode=\"w\") as f_keypoints, \\\n",
    "         h5py.File(feature_dir / \"descriptors.h5\", mode=\"w\") as f_descriptors:\n",
    "        \n",
    "        for path in tqdm(paths, desc=\"detect_keypoints\"):\n",
    "#         for path in paths:\n",
    "            \n",
    "            key = path.name\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                image = load_torch_image(path, device=device).to(dtype)\n",
    "                features = extractor.extract(image)\n",
    "                \n",
    "                f_keypoints[key] = features[\"keypoints\"].squeeze().detach().cpu().numpy()\n",
    "                f_descriptors[key] = features[\"descriptors\"].squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680f0c3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:25:31.922800Z",
     "iopub.status.busy": "2024-05-12T18:25:31.922110Z",
     "iopub.status.idle": "2024-05-12T18:25:31.934416Z",
     "shell.execute_reply": "2024-05-12T18:25:31.933521Z"
    },
    "papermill": {
     "duration": 0.022243,
     "end_time": "2024-05-12T18:25:31.936454",
     "exception": false,
     "start_time": "2024-05-12T18:25:31.914211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def keypoint_distances(\n",
    "    paths: list[Path],\n",
    "    index_pairs: list[tuple[int, int]],\n",
    "    feature_dir: Path,\n",
    "    min_matches: int = 15,\n",
    "    verbose: bool = True,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> None:\n",
    "    \"\"\"Computes distances between keypoints of images.\n",
    "    \n",
    "    Stores output at feature_dir/matches.h5\n",
    "    \"\"\"\n",
    "    \n",
    "    matcher_params = {\n",
    "        \"width_confidence\": -1,\n",
    "        \"depth_confidence\": -1,\n",
    "        \"mp\": True if 'cuda' in str(device) else False,\n",
    "    }\n",
    "    matcher = KF.LightGlueMatcher(\"aliked\", matcher_params).eval().to(device)\n",
    "    print(\"computing keypoint_distances\", len(paths), \"len pair\", len(index_pairs))\n",
    "    with h5py.File(feature_dir / \"keypoints.h5\", mode=\"r\") as f_keypoints, \\\n",
    "         h5py.File(feature_dir / \"descriptors.h5\", mode=\"r\") as f_descriptors, \\\n",
    "         h5py.File(feature_dir / \"matches.h5\", mode=\"w\") as f_matches:\n",
    "        \n",
    "            for idx1, idx2 in tqdm(index_pairs, desc=\"keypoint_distances\"):\n",
    "#             for idx1, idx2 in index_pairs:\n",
    "                key1, key2 = paths[idx1].name, paths[idx2].name\n",
    "\n",
    "                keypoints1 = torch.from_numpy(f_keypoints[key1][...]).to(device)\n",
    "                keypoints2 = torch.from_numpy(f_keypoints[key2][...]).to(device)\n",
    "                descriptors1 = torch.from_numpy(f_descriptors[key1][...]).to(device)\n",
    "                descriptors2 = torch.from_numpy(f_descriptors[key2][...]).to(device)\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    distances, indices = matcher(\n",
    "                        descriptors1, \n",
    "                        descriptors2, \n",
    "                        KF.laf_from_center_scale_ori(keypoints1[None]),\n",
    "                        KF.laf_from_center_scale_ori(keypoints2[None]),\n",
    "                    )\n",
    "\n",
    "                # We have matches to consider\n",
    "                n_matches = len(indices)\n",
    "                if n_matches:\n",
    "                    if verbose:\n",
    "                        print(f\"{key1}-{key2}: {n_matches} matches\")\n",
    "                    # Store the matches in the group of one image\n",
    "                    if n_matches >= min_matches:\n",
    "                        group  = f_matches.require_group(key1)\n",
    "                        group.create_dataset(key2, data=indices.detach().cpu().numpy().reshape(-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a00e54bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:25:31.952176Z",
     "iopub.status.busy": "2024-05-12T18:25:31.951930Z",
     "iopub.status.idle": "2024-05-12T18:25:31.956183Z",
     "shell.execute_reply": "2024-05-12T18:25:31.955349Z"
    },
    "papermill": {
     "duration": 0.014271,
     "end_time": "2024-05-12T18:25:31.958141",
     "exception": false,
     "start_time": "2024-05-12T18:25:31.943870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     images_list = list(Path(\"/kaggle/input/image-matching-challenge-2024/test/church/images/\").glob(\"*.png\"))[:10]\n",
    "#     index_pairs = get_image_pairs(images_list, \"dinov2\")\n",
    "#     print(index_pairs)\n",
    "#     feature_dir = Path(\"./sample_test_features\")\n",
    "#     detect_keypoints(images_list, feature_dir)\n",
    "#     keypoint_distances(images_list, index_pairs, feature_dir, verbose=False, device=device)\n",
    "#     for idx1, idx2 in index_pairs:\n",
    "#         key1, key2 = images_list[idx1], images_list[idx2]\n",
    "#         img1 = load_torch_image(key1)\n",
    "#         img2 = load_torch_image(key2)\n",
    "#         fig, ax = plt.subplots(1, 2, figsize=(10, 20))\n",
    "#         ax[0].imshow(img1[0, ...].permute(1,2,0).cpu())\n",
    "#         ax[1].imshow(img2[0, ...].permute(1,2,0).cpu())\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7f72b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:25:31.973535Z",
     "iopub.status.busy": "2024-05-12T18:25:31.973186Z",
     "iopub.status.idle": "2024-05-12T18:25:31.979256Z",
     "shell.execute_reply": "2024-05-12T18:25:31.978492Z"
    },
    "papermill": {
     "duration": 0.015826,
     "end_time": "2024-05-12T18:25:31.981101",
     "exception": false,
     "start_time": "2024-05-12T18:25:31.965275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_into_colmap(\n",
    "    path: Path,\n",
    "    feature_dir: Path,\n",
    "    database_path: str = \"colmap.db\",\n",
    ") -> None:\n",
    "    \"\"\"Adds keypoints into colmap\"\"\"\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    path1 = path\n",
    "    path2 = Path(str(path1).replace(\"input/image-matching-challenge-2024\", \"working\"))\n",
    "    orin_image_name = os.listdir(path1)\n",
    "    fname_to_id = add_keypoints(db, feature_dir, path1, path2, orin_image_name, \"\", \"simple-pinhole\", single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100651fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:25:31.996474Z",
     "iopub.status.busy": "2024-05-12T18:25:31.996146Z",
     "iopub.status.idle": "2024-05-12T18:25:32.002841Z",
     "shell.execute_reply": "2024-05-12T18:25:32.001922Z"
    },
    "papermill": {
     "duration": 0.016537,
     "end_time": "2024-05-12T18:25:32.004866",
     "exception": false,
     "start_time": "2024-05-12T18:25:31.988329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    base_path: Path = Path(\"/kaggle/input/image-matching-challenge-2024/\")\n",
    "#     base_path: Path = Path(\"/kaggle/input/custom-dataset\")\n",
    "        \n",
    "    feature_dir: Path = Path.cwd() / \".feature_outputs\"\n",
    "        \n",
    "    device: torch.device = K.utils.get_cuda_device_if_available(0)\n",
    "    \n",
    "    pair_matching_args = {\n",
    "        \"model_name\": \"/kaggle/input/dinov2/pytorch/large/1\",\n",
    "        \"similarity_threshold\": 0.6,\n",
    "        \"tolerance\": 500,\n",
    "        \"min_matches\": 50,\n",
    "        \"exhaustive_if_less\": 50,\n",
    "        \"p\": 2.0,\n",
    "    }\n",
    "    \n",
    "    keypoint_detection_args = {\n",
    "        \"num_features\": 4096,\n",
    "        \"resize_to\": 1024,\n",
    "    }\n",
    "    \n",
    "    keypoint_distances_args = {\n",
    "        \"min_matches\": 15,\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "    \n",
    "    colmap_mapper_options = {\n",
    "        \"min_model_size\": 3, # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "        \"max_num_models\": 2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e89025d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:25:32.020808Z",
     "iopub.status.busy": "2024-05-12T18:25:32.020546Z",
     "iopub.status.idle": "2024-05-12T18:25:32.035773Z",
     "shell.execute_reply": "2024-05-12T18:25:32.035037Z"
    },
    "papermill": {
     "duration": 0.02542,
     "end_time": "2024-05-12T18:25:32.037683",
     "exception": false,
     "start_time": "2024-05-12T18:25:32.012263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_sample_submission() -> dict[dict[str, list[Path]]]:\n",
    "    \"\"\"Construct a dict describing the test data as \n",
    "    \n",
    "    {\"dataset\": {\"scene\": [<image paths>]}}\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    with open(\"sample_submission.csv\", \"r\") as f:\n",
    "        for i, l in enumerate(f):\n",
    "            # Skip header\n",
    "            if i == 0:\n",
    "                print(\"header:\", l)\n",
    "\n",
    "            if l and i > 0:\n",
    "                image_path, dataset, scene, _, _ = l.strip().split(',')\n",
    "                if dataset not in data_dict:\n",
    "                    data_dict[dataset] = {}\n",
    "                if scene not in data_dict[dataset]:\n",
    "                    data_dict[dataset][scene] = []\n",
    "                if image_path in orin_image_path:\n",
    "                    base_path = Path(\"/kaggle/input/image-matching-challenge-2024/\")\n",
    "                else:\n",
    "                    base_path = Path(\"/kaggle/working/\")\n",
    "                data_dict[dataset][scene].append(Path(base_path / image_path))\n",
    "\n",
    "    for dataset in data_dict:\n",
    "        for scene in data_dict[dataset]:\n",
    "            print(f\"{dataset} / {scene} -> {len(data_dict[dataset][scene])} images\")\n",
    "\n",
    "    return data_dict\n",
    "def create_submission(\n",
    "    results: dict,\n",
    "    data_dict: dict[dict[str, list[Path]]],\n",
    "    base_path: Path,\n",
    "    orin_image_path\n",
    ") -> None:\n",
    "    \"\"\"Prepares a submission file.\"\"\"\n",
    "    \n",
    "    with open(\"submission.csv\", \"w\") as f:\n",
    "        f.write(\"image_path,dataset,scene,rotation_matrix,translation_vector\\n\")\n",
    "        \n",
    "        for dataset in data_dict:\n",
    "            # Only write results for datasets with images that have results \n",
    "            if dataset in results:\n",
    "                res = results[dataset]\n",
    "            else:\n",
    "                res = {}\n",
    "            \n",
    "            # Same for scenes\n",
    "            for scene in data_dict[dataset]:\n",
    "                if scene in res:\n",
    "                    scene_res = res[scene]\n",
    "                else:\n",
    "                    scene_res = {\"R\":{}, \"t\":{}}\n",
    "                scene_res_2 = {}\n",
    "                for skey in scene_res.keys():\n",
    "                    idx = str(skey).find(\"test\")\n",
    "                \n",
    "                    skey2 = str(skey.relative_to(base_path))\n",
    "                    scene_res_2[skey2] = scene_res[skey]\n",
    "                    \n",
    "                # Write the row with rotation and translation matrices\n",
    "                for image in data_dict[dataset][scene]:\n",
    "                    image = str(image)\n",
    "                    idx = image.find(\"test\")\n",
    "                    image = image[idx:]\n",
    "                    if image in scene_res_2:\n",
    "                        print(image)\n",
    "                        R = scene_res_2[image][\"R\"].reshape(-1)\n",
    "                        T = scene_res_2[image][\"t\"].reshape(-1)\n",
    "                    else:\n",
    "                        R = np.eye(3).reshape(-1)\n",
    "                        T = np.zeros((3))\n",
    "                    if image in orin_image_path:\n",
    "                        f.write(f\"{image},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba1e079e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:25:32.052985Z",
     "iopub.status.busy": "2024-05-12T18:25:32.052720Z",
     "iopub.status.idle": "2024-05-12T18:25:32.056404Z",
     "shell.execute_reply": "2024-05-12T18:25:32.055660Z"
    },
    "papermill": {
     "duration": 0.013498,
     "end_time": "2024-05-12T18:25:32.058345",
     "exception": false,
     "start_time": "2024-05-12T18:25:32.044847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6096a056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:25:32.074704Z",
     "iopub.status.busy": "2024-05-12T18:25:32.074434Z",
     "iopub.status.idle": "2024-05-12T18:35:17.083627Z",
     "shell.execute_reply": "2024-05-12T18:35:17.082715Z"
    },
    "papermill": {
     "duration": 585.020346,
     "end_time": "2024-05-12T18:35:17.086279",
     "exception": false,
     "start_time": "2024-05-12T18:25:32.065933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for the best reconstruction\n",
      "0 Reconstruction:\n",
      "\tnum_reg_images = 65\n",
      "\tnum_cameras = 65\n",
      "\tnum_points3D = 19032\n",
      "\tnum_observations = 119394\n",
      "\tmean_track_length = 6.27333\n",
      "\tmean_observations_per_image = 1836.83\n",
      "\tmean_reprojection_error = 0.959565\n",
      "Registered: church / church -> 65 images\n",
      "Total: church / church -> 65 images\n",
      "test/church/images/00046.png\n",
      "test/church/images/00090.png\n",
      "test/church/images/00092.png\n",
      "test/church/images/00087.png\n",
      "test/church/images/00050.png\n",
      "test/church/images/00068.png\n",
      "test/church/images/00083.png\n",
      "test/church/images/00096.png\n",
      "test/church/images/00069.png\n",
      "test/church/images/00081.png\n",
      "test/church/images/00042.png\n",
      "test/church/images/00018.png\n",
      "test/church/images/00030.png\n",
      "test/church/images/00024.png\n",
      "test/church/images/00032.png\n",
      "test/church/images/00026.png\n",
      "test/church/images/00037.png\n",
      "test/church/images/00008.png\n",
      "test/church/images/00035.png\n",
      "test/church/images/00021.png\n",
      "test/church/images/00010.png\n",
      "test/church/images/00039.png\n",
      "test/church/images/00011.png\n",
      "test/church/images/00013.png\n",
      "test/church/images/00006.png\n",
      "test/church/images/00012.png\n",
      "test/church/images/00029.png\n",
      "test/church/images/00001.png\n",
      "test/church/images/00098.png\n",
      "test/church/images/00072.png\n",
      "test/church/images/00066.png\n",
      "test/church/images/00104.png\n",
      "test/church/images/00058.png\n",
      "test/church/images/00059.png\n",
      "test/church/images/00111.png\n",
      "test/church/images/00061.png\n",
      "test/church/images/00060.png\n",
      "test/church/images/00074.png\n",
      "test/church/images/00102.png\n",
      "test/church/images/00076.png\n",
      "test/church/images/00063.png\n",
      "test/church/images/00073.png\n",
      "test/church/images/00044.png\n",
      "test/church/images/00003.png\n",
      "test/church/images/00093.png\n",
      "test/church/images/00079.png\n",
      "test/church/images/00056.png\n",
      "test/church/images/00014.png\n",
      "test/church/images/00009.png\n",
      "test/church/images/00041.png\n",
      "test/church/images/00015.png\n",
      "test/church/images/00052.png\n",
      "test/church/images/00088.png\n",
      "test/church/images/00004.png\n",
      "test/church/images/00070.png\n",
      "test/church/images/00048.png\n",
      "test/church/images/00064.png\n",
      "test/church/images/00075.png\n",
      "test/church/images/00057.png\n",
      "test/church/images/00022.png\n",
      "test/church/images/00043.png\n",
      "test/church/images/00051.png\n",
      "test/church/images/00020.png\n",
      "test/church/images/00101.png\n",
      "test/church/images/00085.png\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "data_dict = parse_sample_submission()\n",
    "datasets = list(data_dict.keys())\n",
    "\n",
    "for dataset in datasets:\n",
    "    if dataset not in results:\n",
    "        results[dataset] = {}\n",
    "\n",
    "    for scene in data_dict[dataset]:\n",
    "        images_dir = data_dict[dataset][scene][0].parent\n",
    "        results[dataset][scene] = {}\n",
    "        image_paths = data_dict[dataset][scene]\n",
    "        print (f\"{scene}: Got {len(image_paths)} images\")\n",
    "\n",
    "        try:\n",
    "            feature_dir = Config.feature_dir / f\"{dataset}_{scene}\"\n",
    "            feature_dir.mkdir(parents=True, exist_ok=True)\n",
    "            database_path = feature_dir / \"colmap.db\"\n",
    "            if database_path.exists():\n",
    "                database_path.unlink()\n",
    "\n",
    "            # 1. Get the pairs of images that are somewhat similar\n",
    "            index_pairs = get_image_pairs(\n",
    "                image_paths,\n",
    "                **Config.pair_matching_args,\n",
    "                device=Config.device,\n",
    "            )\n",
    "            gc.collect()\n",
    "\n",
    "            # 2. Detect keypoints of all images\n",
    "            detect_keypoints(\n",
    "                image_paths,\n",
    "                feature_dir,\n",
    "                **Config.keypoint_detection_args,\n",
    "                device=device,\n",
    "            )\n",
    "            gc.collect()\n",
    "\n",
    "            # 3. Match  keypoints of pairs of similar images\n",
    "            keypoint_distances(\n",
    "                image_paths, \n",
    "                index_pairs, \n",
    "                feature_dir,\n",
    "                **Config.keypoint_distances_args,\n",
    "                device=device,\n",
    "            )\n",
    "            gc.collect()\n",
    "\n",
    "            sleep(1)\n",
    "\n",
    "            # 4.1. Import keypoint distances of matches into colmap for RANSAC \n",
    "            import_into_colmap(\n",
    "                images_dir, \n",
    "                feature_dir, \n",
    "                database_path,\n",
    "            )\n",
    "\n",
    "            output_path = feature_dir / \"colmap_rec_aliked\"\n",
    "            output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # 4.2. Compute RANSAC (detect match outliers)\n",
    "            # By doing it exhaustively we guarantee we will find the best possible Configuration\n",
    "            pycolmap.match_exhaustive(database_path)\n",
    "\n",
    "            mapper_options = pycolmap.IncrementalPipelineOptions(**Config.colmap_mapper_options)\n",
    "\n",
    "            # 5.1 Incrementally start reconstructing the scene (sparse reconstruction)\n",
    "            # The process starts from a random pair of images and is incrementally extended by \n",
    "            # registering new images and triangulating new points.\n",
    "            maps = pycolmap.incremental_mapping(\n",
    "                database_path=database_path, \n",
    "                image_path=images_dir,\n",
    "                output_path=output_path, \n",
    "                options=mapper_options,\n",
    "            )\n",
    "\n",
    "#             print(maps)\n",
    "            clear_output(wait=False)\n",
    "\n",
    "            # 5.2. Look for the best reconstruction: The incremental mapping offered by \n",
    "            # pycolmap attempts to reconstruct multiple models, we must pick the best one\n",
    "            images_registered  = 0\n",
    "            best_idx = None\n",
    "\n",
    "            print (\"Looking for the best reconstruction\")\n",
    "\n",
    "            if isinstance(maps, dict):\n",
    "                for idx1, rec in maps.items():\n",
    "                    print(idx1, rec.summary())\n",
    "                    try:\n",
    "                        if len(rec.images) > images_registered:\n",
    "                            images_registered = len(rec.images)\n",
    "                            best_idx = idx1\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "            # Parse the reconstruction object to get the rotation matrix and translation vector\n",
    "            # obtained for each image in the reconstruction\n",
    "            if best_idx is not None:\n",
    "                for k, im in maps[best_idx].images.items():\n",
    "                    key = Config.base_path / \"test\" / scene / \"images\" / im.name\n",
    "                    results[dataset][scene][key] = {}\n",
    "                    results[dataset][scene][key][\"R\"] = deepcopy(im.cam_from_world.rotation.matrix())\n",
    "                    results[dataset][scene][key][\"t\"] = deepcopy(np.array(im.cam_from_world.translation))\n",
    "\n",
    "            print(f\"Registered: {dataset} / {scene} -> {len(results[dataset][scene])} images\")\n",
    "            print(f\"Total: {dataset} / {scene} -> {len(data_dict[dataset][scene])} images\")\n",
    "            create_submission(results, data_dict, Config.base_path, orin_image_path)\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33741498",
   "metadata": {
    "papermill": {
     "duration": 0.006803,
     "end_time": "2024-05-12T18:35:17.100892",
     "exception": false,
     "start_time": "2024-05-12T18:35:17.094089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e572cf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:35:17.116429Z",
     "iopub.status.busy": "2024-05-12T18:35:17.116115Z",
     "iopub.status.idle": "2024-05-12T18:35:17.176899Z",
     "shell.execute_reply": "2024-05-12T18:35:17.176151Z"
    },
    "papermill": {
     "duration": 0.071098,
     "end_time": "2024-05-12T18:35:17.178779",
     "exception": false,
     "start_time": "2024-05-12T18:35:17.107681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "\n",
    "_EPS = np.finfo(float).eps * 4.0\n",
    "\n",
    "# mAA evaluation thresholds per scene, different accoring to the scene\n",
    "translation_thresholds_meters_dict = {\n",
    " 'multi-temporal-temple-baalshamin':  np.array([0.025,  0.05,  0.1,  0.2,  0.5,  1.0]),\n",
    " 'pond':                              np.array([0.025,  0.05,  0.1,  0.2,  0.5,  1.0]),\n",
    " 'transp_obj_glass_cylinder':         np.array([0.0025, 0.005, 0.01, 0.02, 0.05, 0.1]),\n",
    " 'transp_obj_glass_cup':              np.array([0.0025, 0.005, 0.01, 0.02, 0.05, 0.1]),\n",
    " 'church':                            np.array([0.025,  0.05,  0.1,  0.2,  0.5,  1.0]),\n",
    " 'lizard':                            np.array([0.025,  0.05,  0.1,  0.2,  0.5,  1.0]),\n",
    " 'dioscuri':                          np.array([0.025,  0.05,  0.1,  0.2,  0.5,  1.0]), \n",
    "}\n",
    "\n",
    "\n",
    "def vector_norm(data, axis=None, out=None):\n",
    "    '''Return length, i.e. Euclidean norm, of ndarray along axis.'''\n",
    "    data = np.array(data, dtype=np.float64, copy=True)\n",
    "    if out is None:\n",
    "        if data.ndim == 1:\n",
    "            return math.sqrt(np.dot(data, data))\n",
    "        data *= data\n",
    "        out = np.atleast_1d(np.sum(data, axis=axis))\n",
    "        np.sqrt(out, out)\n",
    "        return out\n",
    "    data *= data\n",
    "    np.sum(data, axis=axis, out=out)\n",
    "    np.sqrt(out, out)\n",
    "    return None\n",
    "\n",
    "\n",
    "def quaternion_matrix(quaternion):\n",
    "    '''Return homogeneous rotation matrix from quaternion.'''\n",
    "    q = np.array(quaternion, dtype=np.float64, copy=True)\n",
    "    n = np.dot(q, q)\n",
    "    if n < _EPS:\n",
    "        # print(\"special case\")\n",
    "        return np.identity(4)\n",
    "    q *= math.sqrt(2.0 / n)\n",
    "    q = np.outer(q, q)\n",
    "    return np.array(\n",
    "        [\n",
    "            [\n",
    "                1.0 - q[2, 2] - q[3, 3],\n",
    "                q[1, 2] - q[3, 0],\n",
    "                q[1, 3] + q[2, 0],\n",
    "                0.0,\n",
    "            ],\n",
    "            [\n",
    "                q[1, 2] + q[3, 0],\n",
    "                1.0 - q[1, 1] - q[3, 3],\n",
    "                q[2, 3] - q[1, 0],\n",
    "                0.0,\n",
    "            ],\n",
    "            [\n",
    "                q[1, 3] - q[2, 0],\n",
    "                q[2, 3] + q[1, 0],\n",
    "                1.0 - q[1, 1] - q[2, 2],\n",
    "                0.0,\n",
    "            ],\n",
    "            [0.0, 0.0, 0.0, 1.0],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# based on the 3D registration from https://github.com/cgohlke/transformations\n",
    "def affine_matrix_from_points(v0, v1, shear=False, scale=True, usesvd=True):\n",
    "    '''Return affine transform matrix to register two point sets.\n",
    "    v0 and v1 are shape (ndims, -1) arrays of at least ndims non-homogeneous\n",
    "    coordinates, where ndims is the dimensionality of the coordinate space.\n",
    "    If shear is False, a similarity transformation matrix is returned.\n",
    "    If also scale is False, a rigid/Euclidean traffansformation matrix\n",
    "    is returned.\n",
    "    By default the algorithm by Hartley and Zissermann [15] is used.\n",
    "    If usesvd is True, similarity and Euclidean transformation matrices\n",
    "    are calculated by minimizing the weighted sum of squared deviations\n",
    "    (RMSD) according to the algorithm by Kabsch [8].\n",
    "    Otherwise, and if ndims is 3, the quaternion based algorithm by Horn [9]\n",
    "    is used, which is slower when using this Python implementation.\n",
    "    The returned matrix performs rotation, translation and uniform scaling\n",
    "    (if specified).'''\n",
    "    \n",
    "    v0 = np.array(v0, dtype=np.float64, copy=True)\n",
    "    v1 = np.array(v1, dtype=np.float64, copy=True)\n",
    "\n",
    "    ndims = v0.shape[0]\n",
    "    if ndims < 2 or v0.shape[1] < ndims or v0.shape != v1.shape:\n",
    "        raise ValueError(\"input arrays are of wrong shape or type\")\n",
    "\n",
    "    # move centroids to origin\n",
    "    t0 = -np.mean(v0, axis=1)\n",
    "    M0 = np.identity(ndims + 1)\n",
    "    M0[:ndims, ndims] = t0\n",
    "    v0 += t0.reshape(ndims, 1)\n",
    "    t1 = -np.mean(v1, axis=1)\n",
    "    M1 = np.identity(ndims + 1)\n",
    "    M1[:ndims, ndims] = t1\n",
    "    v1 += t1.reshape(ndims, 1)\n",
    "\n",
    "    if shear:\n",
    "        # Affine transformation\n",
    "        A = np.concatenate((v0, v1), axis=0)\n",
    "        u, s, vh = np.linalg.svd(A.T)\n",
    "        vh = vh[:ndims].T\n",
    "        B = vh[:ndims]\n",
    "        C = vh[ndims: 2 * ndims]\n",
    "        t = np.dot(C, np.linalg.pinv(B))\n",
    "        t = np.concatenate((t, np.zeros((ndims, 1))), axis=1)\n",
    "        M = np.vstack((t, ((0.0,) * ndims) + (1.0,)))\n",
    "    elif usesvd or ndims != 3:\n",
    "        # Rigid transformation via SVD of covariance matrix\n",
    "        u, s, vh = np.linalg.svd(np.dot(v1, v0.T))\n",
    "        # rotation matrix from SVD orthonormal bases\n",
    "        R = np.dot(u, vh)\n",
    "        if np.linalg.det(R) < 0.0:\n",
    "            # R does not constitute right handed system\n",
    "            R -= np.outer(u[:, ndims - 1], vh[ndims - 1, :] * 2.0)\n",
    "            s[-1] *= -1.0\n",
    "        # homogeneous transformation matrix\n",
    "        M = np.identity(ndims + 1)\n",
    "        M[:ndims, :ndims] = R\n",
    "    else:\n",
    "        # Rigid transformation matrix via quaternion\n",
    "        # compute symmetric matrix N\n",
    "        xx, yy, zz = np.sum(v0 * v1, axis=1)\n",
    "        xy, yz, zx = np.sum(v0 * np.roll(v1, -1, axis=0), axis=1)\n",
    "        xz, yx, zy = np.sum(v0 * np.roll(v1, -2, axis=0), axis=1)\n",
    "        N = [\n",
    "            [xx + yy + zz, 0.0, 0.0, 0.0],\n",
    "            [yz - zy, xx - yy - zz, 0.0, 0.0],\n",
    "            [zx - xz, xy + yx, yy - xx - zz, 0.0],\n",
    "            [xy - yx, zx + xz, yz + zy, zz - xx - yy],\n",
    "        ]\n",
    "        # quaternion: eigenvector corresponding to most positive eigenvalue\n",
    "        w, V = np.linalg.eigh(N)\n",
    "        q = V[:, np.argmax(w)]\n",
    "        # print (vector_norm(q), np.linalg.norm(q))\n",
    "        q /= vector_norm(q)  # unit quaternion\n",
    "        # homogeneous transformation matrix\n",
    "        M = quaternion_matrix(q)\n",
    "\n",
    "    if scale and not shear:\n",
    "        # Affine transformation; scale is ratio of RMS deviations from centroid\n",
    "        v0 *= v0\n",
    "        v1 *= v1\n",
    "        M[:ndims, :ndims] *= math.sqrt(np.sum(v1) / np.sum(v0))\n",
    "\n",
    "    # move centroids back\n",
    "    M = np.dot(np.linalg.inv(M1), np.dot(M, M0))\n",
    "    M /= M[ndims, ndims]\n",
    "\n",
    "    # print(\"transformation matrix Python Script: \", M)\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "# This is the IMC 3D error metric code\n",
    "def register_by_Horn(ev_coord, gt_coord, ransac_threshold, inl_cf, strict_cf):\n",
    "    '''Return the best similarity transforms T that registers 3D points pt_ev in <ev_coord> to\n",
    "    the corresponding ones pt_gt in <gt_coord> according to a RANSAC-like approach for each\n",
    "    threshold value th in <ransac_threshold>.\n",
    "    \n",
    "    Given th, each triplet of 3D correspondences is examined if not already present as strict inlier,\n",
    "    a correspondence is a strict inlier if <strict_cf> * err_best < th, where err_best is the registration\n",
    "    error for the best model so far.\n",
    "    The minimal model given by the triplet is then refined using also its inliers if their total is greater\n",
    "    than <inl_cf> * ninl_best, where ninl_best is th number of inliers for the best model so far. Inliers\n",
    "    are 3D correspondences (pt_ev, pt_gt) for which the Euclidean distance |pt_gt-T*pt_ev| is less than th.'''\n",
    "    \n",
    "    # remove invalid cameras, the index is returned\n",
    "    idx_cams = np.all(np.isfinite(ev_coord), axis=0)\n",
    "    ev_coord = ev_coord[:, idx_cams]\n",
    "    gt_coord = gt_coord[:, idx_cams]\n",
    "\n",
    "    # initialization\n",
    "    n = ev_coord.shape[1]\n",
    "    r = ransac_threshold.shape[0]\n",
    "    ransac_threshold = np.expand_dims(ransac_threshold, axis=0)\n",
    "    ransac_threshold2 = ransac_threshold**2\n",
    "    ev_coord_1 = np.vstack((ev_coord, np.ones(n)))\n",
    "\n",
    "    max_no_inl = np.zeros((1, r))\n",
    "    best_inl_err = np.full(r, np.Inf)\n",
    "    best_transf_matrix = np.zeros((r, 4, 4))\n",
    "    best_err = np.full((n, r), np.Inf)\n",
    "    strict_inl = np.full((n, r), False)\n",
    "    triplets_used = np.zeros((3, r))\n",
    "\n",
    "    # run on camera triplets\n",
    "    for ii in range(n-2):\n",
    "        for jj in range(ii+1, n-1):\n",
    "            for kk in range(jj+1, n):\n",
    "                i = [ii, jj, kk]\n",
    "                triplets_used_now = np.full((n), False)\n",
    "                triplets_used_now[i] = True\n",
    "                # if both ii, jj, kk are strict inliers for the best current model just skip\n",
    "                if np.all(strict_inl[i]):\n",
    "                    continue\n",
    "                # get transformation T by Horn on the triplet camera center correspondences\n",
    "                transf_matrix = affine_matrix_from_points(ev_coord[:, i], gt_coord[:, i], usesvd=False)\n",
    "                # apply transformation T to test camera centres\n",
    "                rotranslated = np.matmul(transf_matrix[:3], ev_coord_1)\n",
    "                # compute error and inliers\n",
    "                err = np.sum((rotranslated - gt_coord)**2, axis=0)\n",
    "                inl = np.expand_dims(err, axis=1) < ransac_threshold2\n",
    "                no_inl = np.sum(inl, axis=0)\n",
    "                # if the number of inliers is close to that of the best model so far, go for refinement\n",
    "                to_ref = np.squeeze(((no_inl > 2) & (no_inl > max_no_inl * inl_cf)), axis=0)\n",
    "                for q in np.argwhere(to_ref):                        \n",
    "                    qq = q[0]\n",
    "                    if np.any(np.all((np.expand_dims(inl[:, qq], axis=1) == inl[:, :qq]), axis=0)):\n",
    "                        # already done for this set of inliers\n",
    "                        continue\n",
    "                    # get transformation T by Horn on the inlier camera center correspondences\n",
    "                    transf_matrix = affine_matrix_from_points(ev_coord[:, inl[:, qq]], gt_coord[:, inl[:, qq]])\n",
    "                    # apply transformation T to test camera centres\n",
    "                    rotranslated = np.matmul(transf_matrix[:3], ev_coord_1)\n",
    "                    # compute error and inliers\n",
    "                    err_ref = np.sum((rotranslated - gt_coord)**2, axis=0)\n",
    "                    err_ref_sum = np.sum(err_ref, axis=0)\n",
    "                    err_ref = np.expand_dims(err_ref, axis=1)\n",
    "                    inl_ref = err_ref < ransac_threshold2\n",
    "                    no_inl_ref = np.sum(inl_ref, axis=0)\n",
    "                    # update the model if better for each threshold\n",
    "                    to_update = np.squeeze((no_inl_ref > max_no_inl) | ((no_inl_ref == max_no_inl) & (err_ref_sum < best_inl_err)), axis=0)\n",
    "                    if np.any(to_update):\n",
    "                        triplets_used[0, to_update] = ii\n",
    "                        triplets_used[1, to_update] = jj\n",
    "                        triplets_used[2, to_update] = kk\n",
    "                        max_no_inl[:, to_update] = no_inl_ref[to_update]\n",
    "                        best_err[:, to_update] = np.sqrt(err_ref)\n",
    "                        best_inl_err[to_update] = err_ref_sum\n",
    "                        strict_inl[:, to_update] = (best_err[:, to_update] < strict_cf * ransac_threshold[:, to_update])\n",
    "                        best_transf_matrix[to_update] = transf_matrix\n",
    "\n",
    "    for i in range(r):\n",
    "       print(f'Registered cameras {int(max_no_inl[0, i])}/{n} for threshold {ransac_threshold[0, i]}')\n",
    "\n",
    "    best_model = {\n",
    "        \"valid_cams\": idx_cams,        \n",
    "        \"no_inl\": max_no_inl,\n",
    "        \"err\": best_err,\n",
    "        \"triplets_used\": triplets_used,\n",
    "        \"transf_matrix\": best_transf_matrix}\n",
    "    return best_model\n",
    "\n",
    "\n",
    "# mAA computation\n",
    "def mAA_on_cameras(err, thresholds, n, skip_top_thresholds, to_dec=3):\n",
    "    '''mAA is the mean of mAA_i, where for each threshold th_i in <thresholds>, excluding the first <skip_top_thresholds values>,\n",
    "    mAA_i = max(0, sum(err_i < th_i) - <to_dec>) / (n - <to_dec>)\n",
    "    where <n> is the number of ground-truth cameras and err_i is the camera registration error for the best \n",
    "    registration corresponding to threshold th_i'''\n",
    "    \n",
    "    aux = err[:, skip_top_thresholds:] < np.expand_dims(np.asarray(thresholds[skip_top_thresholds:]), axis=0)\n",
    "    return np.sum(np.maximum(np.sum(aux, axis=0) - to_dec, 0)) / (len(thresholds[skip_top_thresholds:]) * (n - to_dec))\n",
    "\n",
    "\n",
    "# import data - no error handling in case float(x) fails\n",
    "def get_camera_centers_from_df(df):\n",
    "    out = {}\n",
    "    for row in df.iterrows():\n",
    "        row = row[1]\n",
    "        fname = row['image_path']\n",
    "        R = np.array([float(x) for x in (row['rotation_matrix'].split(';'))]).reshape(3,3)\n",
    "        t = np.array([float(x) for x in (row['translation_vector'].split(';'))]).reshape(3)\n",
    "        center = -R.T @ t\n",
    "        out[fname] = center\n",
    "    return out\n",
    "\n",
    "\n",
    "def evaluate_rec(gt_df, user_df, inl_cf = 0.8, strict_cf=0.5, skip_top_thresholds=2, to_dec=3,\n",
    "                 thresholds=[0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2]):\n",
    "    ''' Register the <user_df> camera centers to the ground-truth <gt_df> camera centers and\n",
    "    return the corresponding mAA as the average percentage of registered camera threshold.\n",
    "    \n",
    "    For each threshold value in <thresholds>, the best similarity transformation found which\n",
    "    maximizes the number of registered cameras is employed. A camera is marked as registered\n",
    "    if after the transformation its Euclidean distance to the corresponding ground-truth camera\n",
    "    center is less than the mentioned threshold. Current measurements are in meter.\n",
    "    \n",
    "    Registration parameters:\n",
    "    <inl_cf> coefficient to activate registration refinement, set to 1 to refine a new model\n",
    "    only when it gives more inliers, to 0 to refine a new model always; high values increase\n",
    "    speed but decrease precision.\n",
    "    <strict_cf> threshold coefficient to define strict inliers for the best registration so far,\n",
    "    new minimal models made up of strict inliers are skipped. It can vary from 0 (slower) to\n",
    "    1 (faster); set to -1 to check exhaustively all the minimal model triplets.\n",
    "\n",
    "    mAA parameters:\n",
    "    <skip_top_thresholds> excluded lower thresholds in the mAA computation; in case of using\n",
    "    heuristics for the registration, i.e. inl_cf!=0 and strict_cf!=-1, best model for lower\n",
    "    threshold can be not the optimal, so skip them in the mAA computation.\n",
    "    <to_dec> excludes the minimal model cameras from the computation of the mAA. Given the\n",
    "    minimal model, i.e. three pairs of 3D correspondences, there is a high chance to register by\n",
    "    a similarity transformation at any threshold, so do not account for mAA'''\n",
    "    \n",
    "    # get camera centers\n",
    "    ucameras = get_camera_centers_from_df(user_df)\n",
    "    gcameras = get_camera_centers_from_df(gt_df)    \n",
    "\n",
    "    # the denominator for mAA ratio\n",
    "    m = gt_df.shape[0]\n",
    "    \n",
    "    # get the image list to use\n",
    "    good_cams = []\n",
    "    for image_path in gcameras.keys():\n",
    "        if image_path in ucameras.keys():\n",
    "            good_cams.append(image_path)\n",
    "    print(\"good_cams\", good_cams)\n",
    "        \n",
    "    # put corresponding camera centers into matrices\n",
    "    n = len(good_cams)\n",
    "    u_cameras = np.zeros((3, n))\n",
    "    g_cameras = np.zeros((3, n))\n",
    "    \n",
    "    ii = 0\n",
    "    for i in good_cams:\n",
    "        u_cameras[:, ii] = ucameras[i]\n",
    "        g_cameras[:, ii] = gcameras[i]\n",
    "        ii += 1\n",
    "        \n",
    "    # Horn camera centers registration, a different best model for each camera threshold\n",
    "    model = register_by_Horn(u_cameras, g_cameras, np.asarray(thresholds), inl_cf, strict_cf)\n",
    "    \n",
    "    # transformation matrix\n",
    "    print(\"\\nTransformation matrix for maximum threshold\")\n",
    "    T = np.squeeze(model['transf_matrix'][-1])\n",
    "    print(T)\n",
    "    \n",
    "    # mAA\n",
    "    mAA = mAA_on_cameras(model[\"err\"], thresholds, m, skip_top_thresholds, to_dec)\n",
    "    # print(f'mAA = {mAA * 100 : .2f}% considering {m} input cameras - {to_dec}')\n",
    "    return mAA\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame) -> float:\n",
    "    '''The metric is an mean average accuracy between solution and submission camera centers.\n",
    "    Prior to calculate the metric, a function performs exhaustive registration (like RANSAC, but\n",
    "    not random, considering all possible configurations) to align the user camera system to the GT'''\n",
    "    \n",
    "    scenes = list(set(solution['dataset'].tolist()))\n",
    "    results_per_dataset = []\n",
    "    for dataset in scenes:\n",
    "        print(f\"\\n*** {dataset} ***\")\n",
    "        start = time.time()\n",
    "        gt_ds = solution[solution['dataset'] == dataset]\n",
    "        user_ds = submission[submission['dataset'] == dataset]\n",
    "        gt_ds = gt_ds.sort_values(by=['image_path'], ascending = True)\n",
    "        user_ds = user_ds.sort_values(by=['image_path'], ascending = True)\n",
    "        result = evaluate_rec(gt_ds, user_ds, inl_cf=0, strict_cf=-1, skip_top_thresholds=0, to_dec=3,\n",
    "                 thresholds=translation_thresholds_meters_dict[dataset])\n",
    "        end = time.time()\n",
    "        print(f\"\\nmAA: {result*100}%\")\n",
    "        print(\"Running time: %s\" % (end - start))        \n",
    "        results_per_dataset.append(result)\n",
    "    return float(np.array(results_per_dataset).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f85056be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:35:17.194039Z",
     "iopub.status.busy": "2024-05-12T18:35:17.193793Z",
     "iopub.status.idle": "2024-05-12T18:35:30.991957Z",
     "shell.execute_reply": "2024-05-12T18:35:30.990893Z"
    },
    "papermill": {
     "duration": 13.810328,
     "end_time": "2024-05-12T18:35:30.996193",
     "exception": false,
     "start_time": "2024-05-12T18:35:17.185865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** church ***\n",
      "good_cams ['00001.png', '00006.png', '00008.png', '00010.png', '00011.png', '00012.png', '00013.png', '00018.png', '00021.png', '00024.png', '00026.png', '00029.png', '00030.png', '00032.png', '00035.png', '00037.png', '00039.png', '00042.png', '00046.png', '00050.png', '00058.png', '00059.png', '00060.png', '00061.png', '00063.png', '00066.png', '00068.png', '00069.png', '00072.png', '00074.png', '00076.png', '00081.png', '00083.png', '00087.png', '00090.png', '00092.png', '00096.png', '00098.png', '00102.png', '00104.png', '00111.png']\n",
      "Registered cameras 4/41 for threshold 0.025\n",
      "Registered cameras 5/41 for threshold 0.05\n",
      "Registered cameras 8/41 for threshold 0.1\n",
      "Registered cameras 13/41 for threshold 0.2\n",
      "Registered cameras 21/41 for threshold 0.5\n",
      "Registered cameras 21/41 for threshold 1.0\n",
      "\n",
      "Transformation matrix for maximum threshold\n",
      "[[ -4.59711811   3.3485602    8.15487069 -21.08112498]\n",
      " [ -3.1639592    7.95759819  -5.05116418  14.158655  ]\n",
      " [ -8.22825216  -4.93072429  -2.61382684  32.01397346]\n",
      " [  0.           0.           0.           1.        ]]\n",
      "\n",
      "mAA: 23.684210526315788%\n",
      "Running time: 13.76826810836792\n",
      "\n",
      "Global mAA: 23.684210526315788%\n",
      "Total running time: 13.768747091293335\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "trans = np.eye(4)\n",
    "trans [:3, -1] = [0,0,0]\n",
    "\n",
    "gt_csv = '/kaggle/input/custom-dataset/test_gt.csv'\n",
    "user_csv = '/kaggle/working/submission.csv'\n",
    "\n",
    "gt_df = pd.read_csv(gt_csv).sort_values(by='image_path')\n",
    "user_df = pd.read_csv(user_csv)\n",
    "\n",
    "user_df['image_path'] = user_df['image_path'].apply(lambda x: os.path.basename(x))\n",
    "user_df = user_df.head(41)\n",
    "user_df = user_df.sort_values(by='image_path')\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "res = score(gt_df, user_df)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"\\nGlobal mAA: {res*100}%\")\n",
    "print(\"Total running time: %s\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f60c78e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:35:31.037370Z",
     "iopub.status.busy": "2024-05-12T18:35:31.036715Z",
     "iopub.status.idle": "2024-05-12T18:35:32.069769Z",
     "shell.execute_reply": "2024-05-12T18:35:32.068659Z"
    },
    "papermill": {
     "duration": 1.056413,
     "end_time": "2024-05-12T18:35:32.072368",
     "exception": false,
     "start_time": "2024-05-12T18:35:31.015955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb  sample_submission.csv  submission.csv  test\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8143495,
     "sourceId": 71885,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4951592,
     "sourceId": 8347846,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 3327,
     "sourceId": 4535,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 636.661608,
   "end_time": "2024-05-12T18:35:35.536892",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-12T18:24:58.875284",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
